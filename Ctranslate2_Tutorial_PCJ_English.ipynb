{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ctranslate2_Tutorial_PCJ_English.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMsehmtN2Nw6",
        "colab_type": "text"
      },
      "source": [
        "Made by Chanjun Park <br>\n",
        "Korea University - Natural Language Prcessing & Artificial Intelligence Lab <br>\n",
        "Email: bcj1210@naver.com <br>\n",
        "CV: https://parkchanjun.github.io/<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5McpoOqV2dGE",
        "colab_type": "text"
      },
      "source": [
        "Ctranslate2? ==> CTranslate2 is a custom C++ inference engine for OpenNMT-py and OpenNMT-tf models supporting both CPU and GPU execution. This project is geared towards efficient serving of standard translation models but is also a place for experimentation around model compression and inference acceleration.<br> \n",
        "\n",
        "***OpenNMT pytorch and Tensorflow both support Transformer models*** <br>\n",
        "***I recommend that you first use OpenNMT pytorch and Tensorflow.***\n",
        "<br>\n",
        "\n",
        "OpenNMT Tutorial: https://github.com/Parkchanjun/OpenNMT-Colab-Tutorial <br><br>\n",
        "\n",
        "Feature is as below <br>\n",
        "\n",
        "**Fast execution**\n",
        "The execution aims to be faster than a general purpose deep learning framework: on standard translation tasks, it is up to 4x faster than OpenNMT-py.<br><br>\n",
        "**Model quantization**\n",
        "Support INT16 quantization on CPU and INT8 quantization (experimental) on CPU and GPU.<br><br>\n",
        "**Parallel translation**\n",
        "Translations can be run efficiently in parallel without duplicating the model data in memory.<br><br>\n",
        "**Dynamic memory usage**\n",
        "The memory usage changes dynamically depending on the request size while still meeting performance requirements thanks to caching allocators on both CPU and GPU.<br><br>\n",
        "**Automatic instruction set dispatch**\n",
        "When using Intel MKL, the dispatch to the optimal instruction set is done at runtime.<br><br>\n",
        "**Ligthweight on disk**\n",
        "Models can be quantized below 100MB with minimal accuracy loss. A full featured Docker image supporting GPU and CPU requires less than 1GB.<br><br>\n",
        "**Easy to use translation APIs**\n",
        "The project exposes translation APIs in Python and C++ to cover most integration needs.<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRh8-8q20hc",
        "colab_type": "text"
      },
      "source": [
        "**1 : Configuration using pip**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj5o9WV-2JR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ctranslate2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUt14UI33Yfv",
        "colab_type": "text"
      },
      "source": [
        "**2: Download the model for the tutorial<br>**\n",
        "In this tutorial, you will use the demo model provided by Ctranslate2, but in practice you will need to use the model that comes directly from OpenNMT Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mmUkaOA20K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://s3.amazonaws.com/opennmt-models/transformer-ende-wmt-pyOnmt.tar.gz\n",
        "!tar xf transformer-ende-wmt-pyOnmt.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muokTI4b3yez",
        "colab_type": "text"
      },
      "source": [
        "**3: Convert the model to a form that Ctranslate2 can use**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cadyJAqB34Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ct2-opennmt-py-converter --model_path averaged-10-epoch.pt --model_spec TransformerBase \\\n",
        "    --output_dir ende_ctranslate2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI2vVbW54C6c",
        "colab_type": "text"
      },
      "source": [
        "You can see that bin and vocab files are created in the ende_ctranslate2 path.<br>\n",
        "The above example is based on OpenNMT-pytorch<br>\n",
        "\n",
        "OpenNMT Tensorflow can be run as follows.\n",
        "\n",
        "ct2-opennmt-tf-converter --model_path averaged-ende-export500k-v2 --model_spec TransformerBase \\\n",
        "    --output_dir ende_ctranslate2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-srLgYwv4mHS",
        "colab_type": "text"
      },
      "source": [
        "**4: Translate - Python API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOVLRDkZ4lRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ctranslate2\n",
        "translator = ctranslate2.Translator(\"ende_ctranslate2/\")\n",
        "outputs = translator.translate_batch([[\"▁H\", \"ello\", \"▁world\", \"!\"]])\n",
        "print(outputs[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jE3NLp94sLT",
        "colab_type": "text"
      },
      "source": [
        "This is an example of translation in Python API form. You can put the sentence to translate in translator.translate_batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45XsTh2r41w0",
        "colab_type": "text"
      },
      "source": [
        "**4단계: Translate - C++ API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk75m-hj41Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#include <iostream>\n",
        "#include <ctranslate2/translator.h>\n",
        "\n",
        "int main() {\n",
        "  ctranslate2::Translator translator(\"ende_ctranslate2/\");\n",
        "  ctranslate2::TranslationResult result = translator.translate({\"▁H\", \"ello\", \"▁world\", \"!\"});\n",
        "\n",
        "  for (const auto& token : result.output())\n",
        "    std::cout << token << ' ';\n",
        "  std::cout << std::endl;\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqgJpm234-KK",
        "colab_type": "text"
      },
      "source": [
        "An example of translating into C ++ is shown above. Just put the sentence you want to translate into translator.translate."
      ]
    }
  ]
}